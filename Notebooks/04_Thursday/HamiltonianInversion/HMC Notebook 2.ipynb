{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../share/skience2020_logo.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     width=\"300\"\n",
    "     style=\"float: right; margin-right: 100px;\" />\n",
    "     \n",
    "# Hamiltonian Monte Carlo - Notebook 2\n",
    "## Bayesian inference on Earthquake hypocenters accelerated with Hamiltonian Monte Carlo\n",
    "\n",
    "##### Authors:\n",
    "* Lars Gebraad ([@larsgeb](https://github.com/larsgeb))\n",
    "\n",
    "##### Authors of the SeismoLive original:\n",
    "* Heiner Igel ([@heinerigel](https://github.com/heinerigel))\n",
    "\n",
    "* Kilian Ge√üele ([@KGessele](https://github.com/KGessele))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll look at cases where HMC is a particular good choice of algorithm for performing Bayesian inference. We will focus on larger datasets; **We'll try to locate multiple Earthquakes at once**, and investigate the benefits and drawbacks of doing this w.r.t. inferring parameters one Eartquake at a time.\n",
    "\n",
    "To do this inference efficiently, I rewrote the forward, misfit and gradient functions to run for multiple Earthquakes at once by **using vectorized expressions**. This results in a significant performance gain, beneficial to both Metropolis-Hastings and Hamiltonian Monte Carlo sampling. These function are given below, but we won't get into the details.\n",
    "\n",
    "Specifically, we do:\n",
    "1. Inverse problem set up, accelerated;\n",
    "2. Battle of the samplers: which sampler does best in limited time?\n",
    "3. Comparison of single event and many event inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Definition of the inverse problem for many Earthquakes\n",
    "\n",
    "This is a quick re-implementation of the functions of notebook 1, optimized for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T22:17:52.979905Z",
     "start_time": "2020-02-13T22:17:52.815285Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# This is a configuration step for the exercise\n",
    "# ---------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.gridspec as _gridspec\n",
    "from typing import List as _List\n",
    "import time\n",
    "\n",
    "# Custom imports from this project\n",
    "from misc import marginal_grid\n",
    "import samplers\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T22:17:52.998165Z",
     "start_time": "2020-02-13T22:17:52.985040Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Forward, misfit and gradients\n",
    "def forward(\n",
    "    m, station_x, station_z,\n",
    "):\n",
    "\n",
    "    assert m.shape == (m.size, 1)\n",
    "\n",
    "    x = m[0:-1:3]\n",
    "    z = m[1:-1:3]\n",
    "    T = m[2:-1:3]\n",
    "    v = m[-1]\n",
    "\n",
    "    t_calc = T + ((x - station_x) ** 2.0 + (z - station_z) ** 2.0) ** 0.5 / v\n",
    "    \n",
    "    t_calc = T + (1.0 / v) * ((x - station_x) ** 2.0 + (z - station_z) ** 2.0) ** 0.5\n",
    "    return t_calc\n",
    "\n",
    "\n",
    "def misfit_opt(\n",
    "    m,\n",
    "    t_obs,  # Observations\n",
    "    sigma_mat,\n",
    "    station_x,\n",
    "    station_z,\n",
    "    depth_limit,  # Prior\n",
    "    v_mean,  # Prior\n",
    "    v_variance,  # Prior\n",
    "):\n",
    "\n",
    "    assert m.shape == (m.size, 1)\n",
    "\n",
    "    x = m[0:-1:3]\n",
    "    z = m[1:-1:3]\n",
    "    T = m[2:-1:3]\n",
    "    v = m[-1]\n",
    "\n",
    "    t_calc = T + ((x - station_x) ** 2.0 + (z - station_z) ** 2.0) ** 0.5 / v\n",
    "\n",
    "    misfit = (t_calc - t_obs) ** 2 / (2 * sigma_mat ** 2)\n",
    "    misfit = np.sum(misfit)\n",
    "    misfit += ((v - v_mean) ** 2) / (2.0 * v_variance)\n",
    "\n",
    "    if np.any(z < 0.0) or np.any(z > depth_limit):\n",
    "        misfit += np.inf\n",
    "\n",
    "    if np.any(x < 0.0) or np.any(x > 30):\n",
    "        misfit += np.inf\n",
    "\n",
    "    return misfit.item()\n",
    "\n",
    "\n",
    "def grad_opt(\n",
    "    m,\n",
    "    t_obs,  # Observations\n",
    "    sigma_mat,\n",
    "    station_x,\n",
    "    station_z,\n",
    "    depth_limit,  # Prior\n",
    "    v_mean,  # Prior\n",
    "    v_variance,  # Prior\n",
    "):\n",
    "\n",
    "    assert m.shape == (m.size, 1)\n",
    "\n",
    "    x = m[0:-1:3]\n",
    "    z = m[1:-1:3]\n",
    "    T = m[2:-1:3]\n",
    "    v = m[-1]\n",
    "\n",
    "    d = ((x - station_x) ** 2.0 + (z - station_z) ** 2.0) ** 0.5\n",
    "\n",
    "    # Data\n",
    "    t_calc = T + d / v\n",
    "\n",
    "    # Data gradients\n",
    "    data_grad_x = (x - station_x) / (v * d)\n",
    "    data_grad_z = (z - station_z) / (v * d)\n",
    "    data_grad_v = -d / (v * v)\n",
    "    data_grad_T = np.ones_like(data_grad_x)\n",
    "\n",
    "    # Misfit gradient\n",
    "    misfit_grad = 2 * (t_calc - t_obs) / (2 * sigma_mat ** 2)\n",
    "\n",
    "    # Applying chain rule\n",
    "    gx = np.sum(misfit_grad * data_grad_x, axis=1)\n",
    "    gz = np.sum(misfit_grad * data_grad_z, axis=1)\n",
    "    gT = np.sum(misfit_grad * data_grad_T, axis=1)\n",
    "    gv = np.sum(misfit_grad * data_grad_v)\n",
    "\n",
    "    # Compiling into total gradient\n",
    "    total_grad = np.zeros_like(m)\n",
    "    total_grad[0:-1:3, 0] = gx\n",
    "    total_grad[1:-1:3, 0] = gz\n",
    "    total_grad[2:-1:3, 0] = gT\n",
    "    total_grad[-1, 0] = gv\n",
    "    total_grad[-1] += (2 * (v - v_mean)) / (2.0 * v_variance)\n",
    "\n",
    "    return total_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T22:17:53.171641Z",
     "start_time": "2020-02-13T22:17:53.164072Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Compilation into a class\n",
    "class target_multiple:\n",
    "    def __init__(\n",
    "        self,\n",
    "        t_obs,\n",
    "        uncertainties,\n",
    "        station_x,\n",
    "        station_z,\n",
    "        v_mean,\n",
    "        v_std,\n",
    "        depth_limit,\n",
    "    ):\n",
    "\n",
    "        # Sanity check\n",
    "        if np.array([station_x, station_z, uncertainties]).size != station_x.size * 3:\n",
    "            print('ERROR: \"station_x, station_z, uncertainties\" must have same length')\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Assign defaults\n",
    "        self.t_obs = t_obs\n",
    "        self.uncertainties = uncertainties\n",
    "        self.sigma_mat = np.tile(\n",
    "            uncertainties, (int(t_obs.size / uncertainties.size), 1)\n",
    "        )\n",
    "        self.station_x = station_x\n",
    "        self.station_z = station_z\n",
    "        self.v_mean = v_mean\n",
    "        self.v_variance = v_std * v_std\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "        # Some householding\n",
    "        self.nevents = int(t_obs.size / uncertainties.size)\n",
    "\n",
    "        self.labels = []\n",
    "        for il in range(self.nevents):\n",
    "            self.labels += [f\"Horizontal source location {il+1}\"]\n",
    "            self.labels += [f\"Vertical source location {il+1}\"]\n",
    "            self.labels += [f\"Origin time {il+1}\"]\n",
    "\n",
    "        self.labels += [\"Medium velocity\"]\n",
    "        self.labels += [\"Misfit\"]\n",
    "        \n",
    "\n",
    "        self.primitive_units = [\"km\", \"km\", \"s\"]\n",
    "        self.units = self.primitive_units * self.nevents + [\n",
    "            \"km/s\",\n",
    "        ] + [\"-\"]\n",
    "\n",
    "    def misfit(self, m):\n",
    "        return misfit_opt(\n",
    "            m,\n",
    "            t_obs=self.t_obs,\n",
    "            sigma_mat=self.sigma_mat,\n",
    "            station_x=self.station_x,\n",
    "            station_z=self.station_z,\n",
    "            v_mean=self.v_mean,\n",
    "            v_variance=self.v_variance,\n",
    "            depth_limit=self.depth_limit,\n",
    "        )\n",
    "\n",
    "    def grad(self, m):\n",
    "        return grad_opt(\n",
    "            m,\n",
    "            t_obs=self.t_obs,\n",
    "            sigma_mat=self.sigma_mat,\n",
    "            station_x=self.station_x,\n",
    "            station_z=self.station_z,\n",
    "            v_mean=self.v_mean,\n",
    "            v_variance=self.v_variance,\n",
    "            depth_limit=self.depth_limit,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'll do now is generate 11 random Eartquakes in addition to the Earthquake from Notebook 1, and assemble the inverse problem with the same prior information as Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T22:17:53.461873Z",
     "start_time": "2020-02-13T22:17:53.455650Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Creating multiple events including the event from Notebook 1\n",
    "\n",
    "# Prior information\n",
    "depth_limit = 25.0\n",
    "v_mean = 4.5\n",
    "v_std = 1.0\n",
    "\n",
    "# We generate 12 random source locations.\n",
    "# We want to recreate the 'random events' every time the same,\n",
    "# so we have to 'seed' the RNG.\n",
    "np.random.seed(42)\n",
    "m_true = []\n",
    "n_events = 12\n",
    "\n",
    "m_true.append(16.0)\n",
    "m_true.append(15.0)\n",
    "m_true.append(17.0)\n",
    "\n",
    "for i in range(n_events - 1):\n",
    "    source_x = np.random.uniform(5, 25)\n",
    "    source_z = np.random.uniform(1, depth_limit - 5)\n",
    "    origin_T = np.random.uniform(4, 20)\n",
    "    m_true.append(source_x)\n",
    "    m_true.append(source_z)\n",
    "    m_true.append(origin_T)\n",
    "    \n",
    "# The last entry of the model vector is the medium velocity\n",
    "v_exact = 5\n",
    "m_true.append(v_exact)\n",
    "\n",
    "# Create true model vector from the list\n",
    "m_true = np.array(m_true, dtype=np.float64)[:, np.newaxis]\n",
    "\n",
    "# Define station coordinates of the array\n",
    "station_x = np.array([0, 15.0, 30.0])\n",
    "station_z = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "# Define uncertainties for the observed arrival time at\n",
    "# each station. These are repeated for all events.\n",
    "uncertainties = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "# Forward model the data\n",
    "t_obs = forward(m_true, station_x, station_z)\n",
    "\n",
    "# Roll everything into a nice tidy object\n",
    "target = target_multiple(\n",
    "    t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T22:17:53.795024Z",
     "start_time": "2020-02-13T22:17:53.612687Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Visualizing the receiver network and true locations\n",
    "fig, axes = plt.subplots(figsize=(6,6))\n",
    "axes.scatter(station_x, station_z, color='b', marker='v', s=200, label='Receivers')\n",
    "axes.scatter(m_true[3:-1:3], m_true[4:-1:3], color='r', marker='.', s=400, label='True locations')\n",
    "axes.scatter(m_true[0], m_true[1], color='g', marker='.', s=400, label='True location from previous notebook')\n",
    "axes = plt.gca()\n",
    "axes.set_xlabel(\"Horizontal location [km]\")\n",
    "axes.set_ylabel(\"Vertical location [km]\")\n",
    "axes.plot([-10, 35], [0, 0], \"k\")\n",
    "axes.set_xlim([-10,35])\n",
    "axes.set_ylim([25,-2])\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our inference is now 12 √ó 3 + 1 = 37 dimensional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Sampling with HMC and MH in limited time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the real test. We have **two major differences** from the sampling test in the previous notebook:\n",
    "\n",
    "1. The sampling is time limited. We give both algorithms 10 seconds to run.\n",
    "2. The HMC algorithm is optimized if the mass matrix is diagonal. This *greatly* speeds up momentum generation.\n",
    "\n",
    "Point 2 might not seem like a fair choice, but actually is. The **MH algorithm allows for exactly the same information injection in the proposal step**, by supplying a covariance matrix to generate the perturbations. However, our implementation already assumed that this covariance matrix was a unit matrix, just like HMC now does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T22:18:05.194322Z",
     "start_time": "2020-02-13T22:17:55.134873Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Perform HMC sampling\n",
    "maxtime = 10.0 # seconds\n",
    "\n",
    "# Starting model\n",
    "m_start = 1.2 * m_true\n",
    "\n",
    "# Tuning parameters\n",
    "mass_matrix = np.eye(m_start.size)\n",
    "epsilon = 0.02\n",
    "nt = 50\n",
    "number_of_samples = 500000\n",
    "\n",
    "# Sampling\n",
    "samples_HMC = samplers.sample_hmc_opt(target, m_start, nt, epsilon, number_of_samples, mass_matrix, maxtime=maxtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:21:09.456691Z",
     "start_time": "2020-02-13T17:20:59.435708Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Perform MH sampling\n",
    "maxtime = 10.0 # seconds\n",
    "\n",
    "# Starting model\n",
    "m_start = 1.2 * m_true\n",
    "\n",
    "# Tuning\n",
    "epsilon = 0.04\n",
    "number_of_samples = 500000\n",
    "\n",
    "# Sampling\n",
    "samples_MH = samplers.sample_mh(target, m_start, epsilon, number_of_samples, maxtime=maxtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about what we expect, the MH algorithm generates about 100x more samples compared to HMC (on my system), but what about the quality?\n",
    "\n",
    "I again precomputed a reference distribution, which is used as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:21:09.461941Z",
     "start_time": "2020-02-13T17:21:09.458415Z"
    }
   },
   "outputs": [],
   "source": [
    "samples_HMC_REF = np.load(\"ref_solutions/twelve_events_reference_HMC_three_receivers.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The moment of truth** is here: which algorithm performs better with the same resources? \n",
    "\n",
    "We start again with the marginals for hypocenter location, and compare them to the reference solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:22:18.057598Z",
     "start_time": "2020-02-13T17:22:14.003793Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting Earthquake locations\n",
    "figure, axess = plt.subplots(target.nevents, 3, figsize=(14, 4 * target.nevents))\n",
    "\n",
    "# Select all samples where speed is fast\n",
    "# selection = samples_HMC[:, samples_HMC[-2,:] < 4]\n",
    "\n",
    "for i in range(target.nevents):\n",
    "    axes = axess[i, 0]\n",
    "    axes.hist2d(\n",
    "        samples_HMC[i * 3, :],\n",
    "        samples_HMC[i * 3 + 1, :],\n",
    "        bins=25,\n",
    "        range=[[0, 35], [-3, depth_limit]],\n",
    "        cmap=plt.get_cmap(\"Greys\"),\n",
    "    )\n",
    "    axes.invert_yaxis()\n",
    "    axes.set_xlabel(\"Horizontal location [km]\")\n",
    "    axes.set_ylabel(\"Vertical location [km]\")\n",
    "\n",
    "    axes.scatter(m_true[i*3], m_true[i*3+1], 200, label=\"True location\")\n",
    "    axes.scatter(station_x, station_z, 200, label=\"Seismographs\", marker='v')\n",
    "    axes.plot([0, 35], [0, 0], \"k\")\n",
    "\n",
    "    axes = axess[i, 1]\n",
    "    axes.hist2d(\n",
    "        samples_MH[i * 3, :],\n",
    "        samples_MH[i * 3 + 1, :],\n",
    "        bins=25,\n",
    "        range=[[0, 35], [-3, depth_limit]],\n",
    "        cmap=plt.get_cmap(\"Greys\"),\n",
    "    )\n",
    "    axes.invert_yaxis()\n",
    "    axes.set_xlabel(\"Horizontal location [km]\")\n",
    "    axes.set_ylabel(\"Vertical location [km]\")\n",
    "\n",
    "    axes.scatter(m_true[i*3], m_true[i*3+1], 200, label=\"True location\")\n",
    "    axes.scatter(station_x, station_z, 200, label=\"Seismographs\", marker='v')\n",
    "    axes.plot([0, 35], [0, 0], \"k\")\n",
    "    \n",
    "    axes = axess[i, 2]\n",
    "    axes.hist2d(\n",
    "        samples_HMC_REF[i * 3, :],\n",
    "        samples_HMC_REF[i * 3 + 1, :],\n",
    "        bins=25,\n",
    "        range=[[0, 35], [-3, depth_limit]],\n",
    "        cmap=plt.get_cmap(\"Greys\"),\n",
    "    )\n",
    "    axes.invert_yaxis()\n",
    "    axes.set_xlabel(\"Horizontal location [km]\")\n",
    "    axes.set_ylabel(\"Vertical location [km]\")\n",
    "\n",
    "    axes.scatter(m_true[i*3], m_true[i*3+1], 200, label=\"True location\")\n",
    "    axes.scatter(station_x, station_z, 200, label=\"Seismographs\", marker='v')\n",
    "    axes.plot([0, 35], [0, 0], \"k\")\n",
    "\n",
    "    \n",
    "axess[0, 0].set_title(\"HMC\")\n",
    "axess[0, 1].set_title(\"Metropolis-Hastings\")\n",
    "axess[0, 2].set_title(\"Reference solution\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be staggering! At least to me they are. HMC, while not being great, is on average much closer to the reference distribution. In the same time, HMC was able to effectively generate more samples from the posterior distribution.\n",
    "\n",
    "All the marginals are visualized below for your reference, but there are many ($12 \\times 3 + 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:21:33.692813Z",
     "start_time": "2020-02-13T17:21:13.329246Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting marginals for all events\n",
    "figure, axes = plt.subplots(\n",
    "    m_start.size, 1, figsize=(8, 3 * m_start.size), # constrained_layout=True\n",
    ")\n",
    "\n",
    "for i in range(m_start.size - 1):\n",
    "    #     axes[i].hist(samples_HMC_REFERENCE[i, :], bins=100, density=True, label=\"REF\");\n",
    "    axes[i].hist(\n",
    "        samples_HMC_REF[i, :],\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=1,\n",
    "        label=\"REFERENCE\",\n",
    "        color=\"k\",\n",
    "    )\n",
    "    axes[i].hist(\n",
    "        samples_HMC[i, :],\n",
    "        bins=10,\n",
    "        density=True,\n",
    "        alpha=1,\n",
    "        linewidth=5,\n",
    "        label=\"HMC\",\n",
    "        histtype=u\"step\",\n",
    "    )\n",
    "    axes[i].hist(\n",
    "        samples_MH[i, :],\n",
    "        bins=10,\n",
    "        density=True,\n",
    "        alpha=1,\n",
    "        linewidth=5,\n",
    "        label=\"MH\",\n",
    "        histtype=u\"step\",\n",
    "    )\n",
    "    ylim = axes[i].get_ylim()\n",
    "    axes[i].plot([m_true[i], m_true[i]], [0, 1], \"k\")\n",
    "    axes[i].set_ylim(ylim)\n",
    "    axes[i].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "    axes[i].set_ylabel(\"Relative likelihood\")\n",
    "    axes[i].legend()\n",
    "\n",
    "i = m_start.size - 1\n",
    "axes[i].hist(\n",
    "    samples_HMC_REF[i, :],\n",
    "    bins=50,\n",
    "    density=True,\n",
    "    alpha=1,\n",
    "    label=\"REFERENCE\",\n",
    "    color=\"k\",\n",
    "    range=(2, 8),\n",
    ")\n",
    "axes[i].hist(\n",
    "    samples_HMC[i, :],\n",
    "    bins=30,\n",
    "    density=True,\n",
    "    alpha=1,\n",
    "    linewidth=5,\n",
    "    label=\"HMC\",\n",
    "    histtype=u\"step\",\n",
    "    range=(2, 8),\n",
    ")\n",
    "axes[i].hist(\n",
    "    samples_MH[i, :],\n",
    "    bins=30,\n",
    "    density=True,\n",
    "    alpha=1,\n",
    "    \n",
    "    linewidth=5,\n",
    "    label=\"MH\",\n",
    "    histtype=u\"step\",\n",
    "    range=(2, 8),\n",
    ")\n",
    "ylim = axes[i].get_ylim()\n",
    "axes[i].plot([m_true[i], m_true[i]], [0, 1], \"k\")\n",
    "axes[i].set_ylim(ylim)\n",
    "axes[i].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "axes[i].set_ylabel(\"Relative likelihood\")\n",
    "axes[i].legend();\n",
    "axes[i].set_xlim([2,8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These marginals confirm that HMC is more efficient with the resources available when the inference problems become bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: What have we learned about the same event and the medium?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event investigated in Notebook 1 is also part of the ensemble of events investigated in this notebook. We can now plot the marginals for these events and compare these to the results of the single event inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:27:46.301664Z",
     "start_time": "2020-02-13T17:27:45.564314Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plotting marginals for event 1 in both single and multi event inference\n",
    "\n",
    "# Loading the solution from the previous notebook\n",
    "samples_HMC_REF_single = np.load(\"ref_solutions/single_event_reference_HMC_three_receivers.npy\")\n",
    "\n",
    "figure, axess = plt.subplots(1, 4, figsize=(14, 4))\n",
    "\n",
    "axess[0].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "for i in range(3):\n",
    "    axess[i].hist(\n",
    "        samples_HMC_REF_single[i, :],\n",
    "        bins=30,\n",
    "        label=\"Single event\",\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "    )\n",
    "    axess[i].hist(\n",
    "        samples_HMC_REF[i, :], bins=30, label=\"Multiple events\", alpha=0.5, density=True\n",
    "    )\n",
    "    axess[i].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "\n",
    "axess[-1].hist(\n",
    "    samples_HMC_REF_single[-2, :],\n",
    "    bins=30,\n",
    "    label=\"Single event\",\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    ")\n",
    "axess[-1].hist(\n",
    "    samples_HMC_REF[-2, 1000:],\n",
    "    bins=30,\n",
    "    label=\"Multiple events\",\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    ")\n",
    "axess[-1].set_xlabel(\"%s [%s]\" % (target.labels[-2], target.units[-2]))\n",
    "\n",
    "axess[1].legend()\n",
    "\n",
    "print(\"Our uncertainties in degrees of belief:\")\n",
    "print(\n",
    "    f\"Standard deviation for origin time (single / many): \"\n",
    "    f\"{np.std(samples_HMC_REF_single[2,:]):.2f} , {np.std(samples_HMC_REF[2,1000:]):.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Standard deviation for medium velocity (single / many): \"\n",
    "    f\"{np.std(samples_HMC_REF_single[-2,:]):.2f} , {np.std(samples_HMC_REF[-2,1000:]):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of parameters has shifted significantly, **especially for origin time and medium velocity**. The spread (uncertainty) in these distributions is reduced. This shows that doing inference for many events at the same time is actually beneficial to our total knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:21:34.768924Z",
     "start_time": "2020-02-13T17:21:34.429223Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of possible event locations for both inferences\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,6))\n",
    "axes[0].hist2d(\n",
    "    samples_HMC_REF[0, :],\n",
    "    samples_HMC_REF[1, :],\n",
    "    bins=40,\n",
    "    range=[[-5, 35], [-3, depth_limit]],\n",
    "    cmap=plt.get_cmap(\"Greys\"),\n",
    ")\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel(\"Horizontal location [km]\")\n",
    "axes[0].set_ylabel(\"Vertical location [km]\")\n",
    "\n",
    "axes[0].plot([-5, 35], [0, 0], \"k\")\n",
    "axes[0].scatter(m_true[0], m_true[1], 200, label=\"True location\")\n",
    "axes[0].scatter(station_x, station_z, 200, label=\"Seismographs\", marker=\"v\")\n",
    "\n",
    "axes[0].set_title(f\"Many ({(samples_HMC_REF[:,0].size-2)/3:.0f}) events simulataneously\")\n",
    "\n",
    "axes[1].hist2d(\n",
    "    samples_HMC_REF_single[0, :],\n",
    "    samples_HMC_REF_single[1, :],\n",
    "    bins=40,\n",
    "    range=[[-5, 35], [-3, depth_limit]],\n",
    "    cmap=plt.get_cmap(\"Greys\"),\n",
    ")\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel(\"Horizontal location [km]\")\n",
    "axes[1].set_ylabel(\"Vertical location [km]\")\n",
    "\n",
    "axes[1].plot([-5, 35], [0, 0], \"k\")\n",
    "axes[1].scatter(m_true[0], m_true[1], 200, label=\"True location\")\n",
    "axes[1].scatter(station_x, station_z, 200, label=\"Seismographs\", marker=\"v\")\n",
    "\n",
    "axes[1].set_title(\"Single event\")\n",
    "\n",
    "axes[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion 2\n",
    "## With more than a few dimensions, HMC gives better results compared to MH when given the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ‚Üì Bonus material is that way ‚Üì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:21:34.775527Z",
     "start_time": "2020-02-13T17:21:34.770801Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Perform HMC sampling\n",
    "maxtime = 60.0 * 60.0  # seconds\n",
    "\n",
    "# Starting model\n",
    "m_start = m_true\n",
    "\n",
    "# Tuning parameters\n",
    "mass_matrix = np.eye(m_start.size)\n",
    "epsilon = 0.03\n",
    "nt = 50\n",
    "number_of_samples = 2000000\n",
    "\n",
    "# Sampling\n",
    "# samples_HMC_REF = samplers.sample_hmc_opt(\n",
    "#     target, m_start, nt, epsilon, number_of_samples, mass_matrix, maxtime=maxtime, thinning=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:21:34.790973Z",
     "start_time": "2020-02-13T17:21:34.776929Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(\"ref_solutions/twelve_events_reference_HMC_three_receivers.npy\", samples_HMC_REF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Checking HMC performance\n",
    " \n",
    " If we want to know how good our Markov chain performs, it's always helpful to look at autocorrelations of a chain. Autocorrelation tells us how much, on average, subsequent samples are related. As we want the Markov chain to generate independent samples, we would like the autocorrelation to be close to zero. The faster this happens (the shorter the lag is for zero autocorrelation) the better a Markov chain performs.\n",
    " \n",
    " For a multivariate Markov chain, we need to analyse the autocorrelations of all parameters. Additionally, cross-autocorrelation plays a role too, but people find this too hard to analyse in general.\n",
    " \n",
    " Below is a small script to visualize the first 30 autocorrelation lags of a Markov chain. If the Markov chain reaches zero AR for all parameters quickly, we are happy. If not, we might like to do some thinning (removing every n-th sample) or tweak our tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:21:37.081068Z",
     "start_time": "2020-02-13T17:21:34.792508Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize autocorrelation\n",
    "thinning = 1 # Use only every n-th sample\n",
    "signal_to_autocorrelate = samples_HMC_REF[:, ::thinning]\n",
    "\n",
    "\n",
    "def mean_correction(vector):\n",
    "    return vector - np.mean(vector)\n",
    "\n",
    "\n",
    "fix, axes = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "maxLag = 30\n",
    "\n",
    "axes.plot([-10, maxLag + 10], [0.1, 0.1], \"--k\", label=\"Assumed decorrelation level\")\n",
    "for i in range(samples_HMC_REF[:, 0].size - 1):\n",
    "    lags, ccs, line, hline = axes.acorr(\n",
    "        signal_to_autocorrelate[i, :],\n",
    "        detrend=mean_correction,\n",
    "        usevlines=False,\n",
    "        maxlags=maxLag,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    axes.plot(lags[int(lags.size / 2) :], ccs[int(lags.size / 2) :], \"--k\", alpha=0.1)\n",
    "axes.set_xlim([-0.2, maxLag])\n",
    "axes.set_xlabel(\"Sample lag\")\n",
    "axes.set_ylabel(\"Autocorrelation\")\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of notebook 2.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "981px",
    "left": "1376px",
    "right": "20px",
    "top": "51px",
    "width": "450px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
