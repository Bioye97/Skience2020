{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: rgba(220,220,220,0.1) ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
    "    <div style=\"float: left ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.9) ; width: 75% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.8) ; line-height: 100%\">Hamiltonian Monte Carlo - Notebook 1</div>\n",
    "            <div style=\"font-size: x-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.8) ; line-height: 100%\">Munich Earth Skience School 2020</div>\n",
    "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.5)\">Bayesian inference on Earthquake hypocenters accelerated with Hamiltonian Monte Carlo</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Authors:\n",
    "* Lars Gebraad ([@larsgeb](https://github.com/larsgeb))\n",
    "\n",
    "##### Authors of the SeismoLive original:\n",
    "* Heiner Igel ([@heinerigel](https://github.com/heinerigel))\n",
    "\n",
    "* Kilian Ge√üele ([@KGessele](https://github.com/KGessele))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be investigating the applicability of **Hamiltonian Monte Carlo** (HMC) to solving non-linear equations and Bayesian inference on their resulting inverse problems. We will look specificly at the Earthquake source-location problem, as presented also on the [SeismoLive website](https://krischer.github.io/seismo_live_build/html/Seismic%20Inverse%20Problems/Earthquake%20Location/el_hypocenter_wrapper.html). \n",
    "\n",
    "In the first section, we'll define the forward and the inverse problem, but this is not the focus of this notebook. These are merely needed to illustrate HMC. \n",
    "\n",
    "We'll look at the following sections:\n",
    "1. Defining the inverse problem;\n",
    "2. An conceptual comparison of Metropolis-Hastings and Hamiltonian Monte Carlo;\n",
    "3. A more rigorous comparison of Metropolis-Hastings and Hamiltonian Monte Carlo;\n",
    "4. A short interpretation of the physical results;\n",
    "5. Influence of the mass matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:02:14.905619Z",
     "start_time": "2020-02-11T14:02:14.675709Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b4384c593067>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msamplers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmarginal_grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\Skience2020\\Notebooks\\04_Thursday\\HamiltonianInversion\\samplers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearSegmentedColormap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "# This is a configuration step for the exercise\n",
    "\n",
    "%matplotlib inline\n",
    "# %load_ext line_profiler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import samplers\n",
    "from misc import marginal_grid\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "font = {'size'   : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to work on a different inverse problem (e.g. Himmelblau sampling), create a class that is structured as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:02:14.909503Z",
     "start_time": "2020-02-11T14:02:14.906785Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class target_example:\n",
    "    def misfit(self, m):\n",
    "        return misfit\n",
    "\n",
    "    def grad(self, m):\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setting up the inverse problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short preparatory section, we will set up a basic hypocenter location inverse problem with unknown medium velocity. We will do this in three steps: \n",
    "\n",
    "1. First, we create a forward model predicting arrival times of earthquakes given a hypocenter location and bulk medium velocity. \n",
    "2. Then we create 'observed' data and together with observational uncertainties and prior information, compile everything into a target object, or posterior distribution. \n",
    "3. Lastly, because we want to perform HMC sampling, we also will define the gradient of the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution of the Forward Problem\n",
    "For any arbitrary model, we can solve the forward problem using the following equation.\n",
    "\n",
    "$$\n",
    "\\mathbf{d} = g(\\mathbf{m})\n",
    "$$\n",
    "\n",
    "where the function $g$ describes the physical processes that associate the model $\\mathbf{m}$ with the theoretical arrival times $\\mathbf{d}$. In this exercise, we use a very simple geometrical concept and assume that the medium is homogeneous and the wave velocity $v$ is constant. Therefore, the theoretical arrival time $t_i$ at one particular station location {$x_i, z_i$} for an arbitrary source {$x, z, T$} in a medium with bulk velocity {$v$} is given by the following equation:\n",
    "\n",
    "\n",
    "$$\n",
    "t_i = g_i(x, z, T, v) = T + \\frac{\\sqrt{(x - x_i)^2 + (x - z_i)^2}}{v}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:10:58.989482Z",
     "start_time": "2020-02-11T14:10:58.975702Z"
    },
    "code_folding": [
     0,
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the forward problem\n",
    "def forward(m, station_x, station_z):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Dissassemble the model vector\n",
    "    x = m[0]\n",
    "    z = m[1]\n",
    "    T = m[2]\n",
    "    v = m[3]\n",
    "\n",
    "    # Create an empty column vector for the data\n",
    "    t_calc = np.empty((station_x.size, 1), dtype=np.float64)\n",
    "\n",
    "    # Loop over the stations\n",
    "    for istat in range(station_x.size):\n",
    "\n",
    "        # Implement the formula for Earthquake first arrival times -------------\n",
    "\n",
    "        t_calc[istat] = None # < here\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # Solution\n",
    "        t_calc[istat] = (\n",
    "            T\n",
    "            + (1.0 / v)\n",
    "            * ((x - station_x[istat]) ** 2.0 + (z - station_z[istat]) ** 2.0) ** 0.5\n",
    "        )\n",
    "\n",
    "    return t_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure of fit\n",
    "\n",
    "To perform an inversions (or inference), we need some criterion that assigns values to each model. This is typically done by comparing the simulated data (synthetics) to the observed data, and mapping these to a scalar function. This function, the misfit, is a measure of how 'good' a model is with respect to the observed data.\n",
    "\n",
    "A typical choice would be the 'L2 misfit'. For the Earthquake source location this is is a quadratic function in residual traveltime (synthetic minus observed). It is given as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{m}, \\mathbf{t}_{obs}) = \n",
    "k\\exp\\Big(-\\frac{1}{2} \\sum_{i}\\big(\\frac{t_i(\\mathbf{m}) - t_i^{obs}}{\\sigma_i}\\big)^2\\Big)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\chi(\\mathbf{m}, \\mathbf{t}_{obs}) = \n",
    "\\frac{1}{2} \\sum_{i}\\left(\\frac{t_i(\\mathbf{m}) - t_i^{obs}}{\\sigma_i}\\right)^2\n",
    "$$  \n",
    "\n",
    "Here we define the Earthquake inversion measure of fit, or misfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:11:00.183325Z",
     "start_time": "2020-02-11T14:11:00.168940Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the misfit of the traveltimes\n",
    "def misfit_tt(\n",
    "    m,\n",
    "    t_obs, # Observations\n",
    "    uncertainties,\n",
    "    station_x, \n",
    "    station_z, \n",
    "    depth_limit, # Prior\n",
    "    v_mean, # Prior\n",
    "    v_std, # Prior\n",
    "):\n",
    "\n",
    "    # Create synthetic data\n",
    "    t_syn = forward(m, station_x, station_z)\n",
    "\n",
    "    # Initialize misfit\n",
    "    misfit = 0.0\n",
    "\n",
    "    # == Likelihood ==\n",
    "    \n",
    "    # Loop over stations\n",
    "    for istat in range(np.size(station_x)):\n",
    "        misfit += ((t_syn[istat] - t_obs[istat]) ** 2) / (\n",
    "            2.0 * uncertainties[istat] * uncertainties[istat]\n",
    "        )\n",
    "\n",
    "    # == Prior == \n",
    "    \n",
    "    z = m[1]\n",
    "    v = m[3]\n",
    "    \n",
    "    # Velocity prior\n",
    "    misfit += ((v - v_mean) ** 2) / (2.0 * v_std * v_std)\n",
    "\n",
    "    # Depth prior\n",
    "    if z < 0.0 or z > depth_limit:\n",
    "        misfit += np.inf\n",
    "\n",
    "    return misfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients of the data and misfit w.r.t. model parameters\n",
    "\n",
    "$$\n",
    "\\chi(\\mathbf{m}, \\mathbf{t}_{obs}) = \n",
    "\\frac{1}{2} \\sum_{i}\\left(\\frac{t_i(\\mathbf{m}) - t_i^{obs}}{\\sigma_i}\\right)^2\n",
    "$$  \n",
    "\n",
    "More explicitly, we have a scalar valued function that depends on the synthetic data, which in turn depends on the model parameters.\n",
    "\n",
    "$$\n",
    "\\chi(\\mathbf{m}) = f\\left(\\mathbf{d}(\\mathbf{m})\\right)\n",
    "$$  \n",
    "\n",
    "Applying the chain rule for the derivative:\n",
    "\n",
    "$$\n",
    "\\partial_\\mathbf{m} \\chi(\\mathbf{m}) = \\left[\\partial_\\mathbf{d} f\\left(\\mathbf{d}(\\mathbf{m})\\right)\\right] \\cdot \\partial_\\mathbf{m}\\mathbf{d}(\\mathbf{m})\n",
    "$$  \n",
    "\n",
    "With nd datapoints and nm model parameters:\n",
    "\n",
    "$$\n",
    "\\partial_\\mathbf{d} f\\left(\\mathbf{d}(\\mathbf{m})\\right) = 1 \\times nd\\\\\n",
    "\\partial_\\mathbf{m}\\mathbf{d}(\\mathbf{m}) = nd \\times nm\n",
    "$$ \n",
    "\n",
    "making the total gradient:\n",
    "\n",
    "$$\n",
    "\\partial_\\mathbf{m} \\chi(\\mathbf{m}) = 1 \\times nm\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:11:04.629491Z",
     "start_time": "2020-02-11T14:11:04.612062Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the gradient of the misfit (and prior) using the chain rule\n",
    "def gradient_data(m, station_x, station_z):\n",
    "    \"\"\"\n",
    "    Returns tensor containing the covariant derivative of the data w.r.t. the model\n",
    "    parameters, i.e. the gradient of a vector function.\n",
    "    \n",
    "    Each column is a parameter, each row a station.\n",
    "    \"\"\"\n",
    "\n",
    "    x = m[0]\n",
    "    z = m[1]\n",
    "    T = m[2]\n",
    "    v = m[3]\n",
    "\n",
    "    grad_t_calc = np.empty((station_x.size, 4), dtype=np.float64)\n",
    "\n",
    "    for istat in range(np.size(station_x)):  # Loop over number of stations\n",
    "\n",
    "        # dd / dx\n",
    "        grad_t_calc[istat, 0] = (1.0 / v) * (\n",
    "            (x - station_x[istat])\n",
    "            / np.sqrt((x - station_x[istat]) ** 2.0 + (z - station_z[istat]) ** 2.0)\n",
    "        )\n",
    "\n",
    "        # dd / dz\n",
    "        grad_t_calc[istat, 1] = (1.0 / v) * (\n",
    "            (z - station_z[istat])\n",
    "            / np.sqrt((x - station_x[istat]) ** 2.0 + (z - station_z[istat]) ** 2.0)\n",
    "        )\n",
    "\n",
    "        # dd / dT\n",
    "        grad_t_calc[istat, 2] = 1\n",
    "\n",
    "        # dd / dv\n",
    "        grad_t_calc[istat, 3] = -(1.0 / (v * v)) * np.sqrt(\n",
    "            (x - station_x[istat]) ** 2.0 + (z - station_z[istat]) ** 2.0\n",
    "        )\n",
    "\n",
    "    return grad_t_calc\n",
    "\n",
    "def gradient_misfit_chain_rule(\n",
    "    m,\n",
    "    t_obs,\n",
    "    uncertainties,\n",
    "    station_x,\n",
    "    station_z,\n",
    "    v_mean,\n",
    "    v_std,\n",
    "    depth_limit,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns covector containing the gradient of the misfit w.r.t. the model\n",
    "    parameters, i.e. the gradient of a scalar function. Implemented using\n",
    "    component wise analytical formulas and the chain rule. \n",
    "    \n",
    "    Each column is a parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    t_syn = forward(m, station_x, station_z)\n",
    "    grad_t_syn = dd_dm(m, station_x, station_z)\n",
    "\n",
    "    grad_misfit = np.empty((1, station_x.size))\n",
    "\n",
    "    for istat in range(np.size(station_x)):\n",
    "\n",
    "        grad_misfit[0, istat] = (\n",
    "            2.0\n",
    "            * (t_syn[istat] - t_obs[istat])\n",
    "            / (2.0 * uncertainties[istat] * uncertainties[istat])\n",
    "        )\n",
    "\n",
    "    grad_misfit = grad_misfit @ grad_t_syn\n",
    "\n",
    "    # Applying prior on velocity\n",
    "    v = m[3]\n",
    "    grad_misfit[0, 3] += 2.0 * (v - v_mean) / (2.0 * v_std * v_std)\n",
    "\n",
    "    return grad_misfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also consider the entire misfit function only a function of model parameters. That just means that the analytical derivatives will be a bit harder. Many times, this is actually beneficial for implementation speed, because implementations can optimize combined floating point operations. An example implementation is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:11:07.621331Z",
     "start_time": "2020-02-11T14:11:07.612812Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the gradient of the misfit (and prior) in one go; analytically\n",
    "def gradient_misfit_analytical(\n",
    "    m, t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns covector containing the gradient of the misfit w.r.t. the model\n",
    "    parameters, i.e. the gradient of a scalar function. Implemented using\n",
    "    analytical derivative.\n",
    "    \n",
    "    Each column is a parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    grad_misfit = np.zeros((1, 4))\n",
    "\n",
    "    x = m[0]\n",
    "    z = m[1]\n",
    "    T = m[2]\n",
    "    v = m[3]\n",
    "\n",
    "    for istat in range(np.size(station_x)):\n",
    "\n",
    "        xs = station_x[istat]\n",
    "        zs = station_z[istat]\n",
    "        t0 = t_obs[istat]\n",
    "\n",
    "        d = ((x - xs) ** 2 + (z - zs) ** 2) ** 0.5\n",
    "\n",
    "        A = 2 * uncertainties[istat] * uncertainties[istat]\n",
    "\n",
    "        grad_misfit[0, 0] += 2 * (x - xs) * (T - t0 + d / v) / (v * d * A)\n",
    "        grad_misfit[0, 1] += 2 * (z - zs) * (T - t0 + d / v) / (v * d * A)\n",
    "        grad_misfit[0, 2] += 2 * (T - t0 + d / v) / A\n",
    "        grad_misfit[0, 3] += -2 * d * (T - t0 + d / v) / (v * v * A)\n",
    "\n",
    "    grad_misfit[0, 3] += 2 * (v - v_mean) / (2 * v_std * v_std)\n",
    "\n",
    "    return grad_misfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling everything into something neat (a class)\n",
    "\n",
    "To make working with the data a little easier, we wrap all the settings associated with the inverse problem into a neat package; a class. This 'target' class only needs:\n",
    "* A constructor to set up;\n",
    "* A target.misfit(m) to evaluate a misfit;\n",
    "* A target.grad(m) to evaluate a gradient.\n",
    "\n",
    "We can just re-use the functions for misfits and gradients we defined in the cells before to create this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:11:07.903440Z",
     "start_time": "2020-02-11T14:11:07.897520Z"
    },
    "code_folding": [
     0,
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the class\n",
    "class target_tt:\n",
    "    def __init__(self, t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit):\n",
    "        \n",
    "        # Sanity check\n",
    "        if np.array([station_x, station_z, uncertainties]).size != station_x.size*3:\n",
    "                print('ERROR: \"station_x, station_z, uncertainties\" must have same length')\n",
    "                raise NotImplementedError\n",
    "        \n",
    "        # Assign defaults\n",
    "        self.t_obs = t_obs\n",
    "        self.uncertainties = uncertainties\n",
    "        self.station_x = station_x\n",
    "        self.station_z = station_z\n",
    "        self.v_mean = v_mean\n",
    "        self.v_std = v_std\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "        # Some householding\n",
    "        self.labels = [\n",
    "            \"Horizontal source location\",\n",
    "            \"Vertical source location\",\n",
    "            \"Origin time\",\n",
    "            \"Medium velocity\",\n",
    "        ]\n",
    "        self.units = [\"km\", \"km\", \"s\", \"km/s\"]\n",
    "\n",
    "    # Optional, could also be programmed directly into misfit\n",
    "    def forward(self, m):\n",
    "        return forward(m, self.station_x, self.station_x)\n",
    "        \n",
    "    def misfit(self, m):\n",
    "        return misfit_tt(\n",
    "            m,\n",
    "            t_obs = self.t_obs,\n",
    "            uncertainties=self.uncertainties,\n",
    "            station_x=self.station_x,\n",
    "            station_z=self.station_z,\n",
    "            v_mean=self.v_mean,\n",
    "            v_std=self.v_std,\n",
    "            depth_limit=self.depth_limit,\n",
    "        )\n",
    "\n",
    "    def grad(self, m):\n",
    "        return gradient_misfit_analytical(\n",
    "            m,\n",
    "            t_obs = self.t_obs,\n",
    "            uncertainties=self.uncertainties,\n",
    "            station_x=self.station_x,\n",
    "            station_z=self.station_z,\n",
    "            v_mean=self.v_mean,\n",
    "            v_std=self.v_std,\n",
    "            depth_limit=self.depth_limit,\n",
    "        ).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating some 'real' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:11:10.218470Z",
     "start_time": "2020-02-11T14:11:10.048137Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the true event properties and receiver network\n",
    "\n",
    "# Define earthquake source properties\n",
    "source_x = 16.0\n",
    "source_z = 15.0\n",
    "origin_T = 17.0\n",
    "v_exact = 5.0\n",
    "\n",
    "# Define station coordinates of the array\n",
    "station_x = np.array([0, 30])\n",
    "station_z = np.array([0.0, 0.0])\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6,6))\n",
    "axes.scatter(station_x, station_z, color='b', marker='v', s=200, label='Receivers')\n",
    "axes.scatter(source_x, source_z, color='r', marker='.', s=400, label='True location')\n",
    "axes = plt.gca()\n",
    "axes.set_xlabel(\"Horizontal location [km]\")\n",
    "axes.set_ylabel(\"Vertical location [km]\")\n",
    "axes.plot([-10, 35], [0, 0], \"k\")\n",
    "axes.set_xlim([-10,35])\n",
    "axes.set_ylim([25,-2])\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:11:25.548332Z",
     "start_time": "2020-02-11T14:11:25.544352Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the true data and obervational uncertainties\n",
    "\n",
    "# Create a vector out of the true values\n",
    "m_true = np.array([source_x, source_z, origin_T, v_exact])\n",
    "\n",
    "# Calculate observed (exact) arrival times for all stations\n",
    "t_obs = forward(m_true, station_x, station_z)\n",
    "\n",
    "# Define uncertainties for the observed arrival time at each station. These are repeated for all events.\n",
    "uncertainties = np.array([0.5, 0.2])\n",
    "\n",
    "assert t_obs.size == uncertainties.size\n",
    "\n",
    "print(\"Observed data (arrival time in seconds w.r.t. the clock on the seismograph):\\n\", t_obs)\n",
    "print(\"Data shape:\\n\", t_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding prior information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have seen in the misfit definition, there are three parameters passed that refer to the prior. These are two parameters for a Gaussian prior on the velocity of the medium, as well as a hard limit for Earthquake source depth. In the actual function, you can see how these add to the misfit.\n",
    "\n",
    "The prior on the Gaussian is a quadratic term which is added to the misfit function, defined by parameters `v_mean` and `v_std`:\n",
    "\n",
    "$$\n",
    "\\chi(\\mathbf{m}) = \\chi_\\text{data}(\\mathbf{m}) + \\frac{(v_\\text{current_model} - v_\\text{mean})^2}{2 v_\\text{std}^2}\n",
    "$$\n",
    "\n",
    "Additionally, the depth of the Earthquake is limited to `depth_limit`, as well as being required to be below the surface of the 'Earth' (i.e. depth $> 0$). We can incorporate these strict boundaries by adding infinity to our misfit term when appropriate:\n",
    "\n",
    "$$\n",
    "\\chi_\\text{final}(\\mathbf{m}) = \\begin{cases}\n",
    "\\chi(\\mathbf{m}) \\quad 0 < \\text{depth} < \\text{depth_limit} \\\\\n",
    "\\infty \\quad else\\\\\n",
    "\\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:11:26.277488Z",
     "start_time": "2020-02-11T14:11:26.272801Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define prior information on depth and velocity\n",
    "v_mean = 4.5\n",
    "v_std = 1\n",
    "depth_limit = 25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the inverse problem 'object' from the class \n",
    "\n",
    "Now we roll everything up into a simple class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:11:26.750753Z",
     "start_time": "2020-02-11T14:11:26.748441Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect the inverse problem into a neat object\n",
    "target = target_tt(t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: An animated comparison of MH and HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior to this notebook, I pre-computed samples by running HMC and MH for longer than 10 minutes. We'll use these samples as a reference distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T17:32:32.062339Z",
     "start_time": "2020-02-11T17:32:32.055717Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a reference distribution. This is only for illustratory purposes.\n",
    "# You could e.g. also use the samples from a previous sampling attempt\n",
    "samples_MH_REF = np.load(\"ref_solutions/single_event_reference_MH.npy\")\n",
    "samples_HMC_REF = np.load(\"ref_solutions/single_event_reference_HMC.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T18:51:39.637045Z",
     "start_time": "2020-02-07T18:51:39.623897Z"
    }
   },
   "source": [
    "## Metropolis-Hastings sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Metropolis-Hastings algorithm is a very fundamental, useful and robust algorithm. The most common variant constructs a Markov chain over our target distribution the following way:\n",
    "\n",
    "1. Start at some initial model $m_\\text{start}$\n",
    "2. Draw a perturbation according to the proposal distribution $dm \\propto P(dm|m_\\text{start})$\n",
    "3. Propose a new model as $m_\\text{start} + dm = m_\\text{new}$\n",
    "4. Check if this new model is likely  (evaluate $P(m_\\text{new})$ ):\n",
    "    * If more likely, move there always.\n",
    "    * If less likely, move there $\\propto dP$\n",
    "5. Repeat by drawing new perturbation\n",
    "\n",
    "Investigating this thing visual is almost always a better idea. The file `samplers.py` provides a 'visual' MH sampler, which plots 2 dimensions of any distribution on the fly. Additionally, it allows us to plot a reference distribution in the background, e.g. a prior or a previous run.\n",
    "\n",
    "In the next notebook cell I load reference samples which I precomputed (from about 10 minutes of sampling) of exactly the same target distribution.\n",
    "\n",
    "**Try changing the step length `epsilon` and see what the algorithm does.** As a guideline, try to get the acceptance rate at 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T17:32:49.630732Z",
     "start_time": "2020-02-11T17:32:36.667784Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform the same sampling using MH, but now animated\n",
    "\n",
    "m_start = np.array([16.1, 15.2, 14.9, 5.7])[:, None]\n",
    "\n",
    "# Tuning parameters =============================================================================================\n",
    "epsilon = 0.2\n",
    "number_of_samples = 100\n",
    "# End ===========================================================================================================\n",
    "\n",
    "# Select which dimensions to animate (2D at most is easiest on computer screens)\n",
    "dims_to_visualize = [2, 3]\n",
    "\n",
    "%matplotlib notebook\n",
    "samples_MH_1 = samplers.visual_sample_mh(\n",
    "    target,\n",
    "    m_start,\n",
    "    epsilon,\n",
    "    number_of_samples,\n",
    "    dims_to_visualize=dims_to_visualize,\n",
    "    background_samples=samples_MH_REF,\n",
    "    true_m=m_true,\n",
    "    animate_sample_interval=1,\n",
    "    animate_proposal=True, # Disable this with 'False' if you find the animation annoying\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see in generating a few samples using MH is that the distance traversed in an accepted move and acceptance rate trade-off; if I try to move greater distances, less samples are accepted. With only 100 samples, we do not approximate the distribution well, regardless of step length.\n",
    "\n",
    "Maybe there is an algorithm that an de better with such few samples ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 'physics': Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen in the presentation that HMC is harder to tune. We have a `mass matrix`, a step length (`dt`) and a number of steps (`nt`). \n",
    "\n",
    "**Try changing the tuning parameters to see what the effect is on the behaviour of the algorithm.** As a guideline, try to get the acceptance rate at 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:13:43.527650Z",
     "start_time": "2020-02-11T14:13:16.578971Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sampling using the basic HMC algorithm, but animated\n",
    "\n",
    "# Select a starting model\n",
    "m_start = np.array([16.1, 15.2, 14.9, 5.7])[:, None]\n",
    "\n",
    "# Choose which dimensions to animate\n",
    "dim_to_vis = [2, 3]\n",
    "\n",
    "# Tuning parameters =============================================================================================\n",
    "# These next two are equivalent\n",
    "# mass_matrix = np.eye(4)\n",
    "mass_matrix = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0],  \n",
    "        [0, 1, 0, 0],  \n",
    "        [0, 0, 1, 0],  \n",
    "        [0, 0, 0, 1],\n",
    "    ], dtype=np.float32\n",
    ")  \n",
    "dt = 0.1\n",
    "nt = 100\n",
    "number_of_samples = 50\n",
    "\n",
    "# Sampling! =====================================================================================================\n",
    "%matplotlib notebook\n",
    "samples_HMC_1 = samplers.visual_sample_hmc(\n",
    "    target,\n",
    "    m_start,\n",
    "    nt,\n",
    "    dt,\n",
    "    number_of_samples,\n",
    "    mass_matrix,\n",
    "    dims_to_visualize=dim_to_vis,\n",
    "    background_samples=samples_MH_REF,\n",
    "    true_m=m_true,\n",
    "    animate_trajectory=True,\n",
    "    animate_trajectory_interval=25, # Lower this number to slow down the animation\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: A qualitative comparison of MH and HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an intuition of what the algorithms do, we can do a little more of a serious comparison. For this, we'll use un-animated algorithms, such that we get **maximum** speed. \n",
    "\n",
    "Additionally, we request 5000 proposals from each algorithm. That does not mean we get 5000 independent samples, just 5000 attempts to move to a new state. First we'll look at the perfomance (the time it takes to generated 5000 samples) and afterwards at the quality of these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:14:32.377961Z",
     "start_time": "2020-02-11T14:14:32.072076Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sampling using the basic MH algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:, None]\n",
    "\n",
    "# Tuning parameters\n",
    "epsilon = 0.3\n",
    "number_of_samples = 5000\n",
    "\n",
    "# Sampling\n",
    "%time samples_MH = samplers.sample_mh(target, m_start, epsilon, number_of_samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:14:51.285326Z",
     "start_time": "2020-02-11T14:14:32.461205Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sampling using the basic HMC algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:,np.newaxis]\n",
    "\n",
    "# Tuning parameters\n",
    "# mass_matrix = np.eye(4)\n",
    "mass_matrix = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "    ], dtype=np.float32\n",
    ")  \n",
    "dt = 0.16\n",
    "nt = 40\n",
    "number_of_samples = 5000\n",
    "\n",
    "# Sampling\n",
    "%time samples_HMC = samplers.sample_hmc(target, m_start, nt, dt, number_of_samples, mass_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in performance for these algorithms is extreme. MH can generate 5000 samples in **under 0.5 seconds** on my machine (a 2018 high end laptop), while HMC takes **about 20 seconds**. \n",
    "\n",
    "But, performance isn't the only thing. The ability of a Markov chain to generate **independent** samples is just as important. To get a feeling of the *quality* of samples, we now compare the resulting two distributions to the reference distribution computed by **many** samples from HMC and MH.\n",
    "\n",
    "The next cell plots the marginals for both chains and compares them to the reference solutions. Additionally, **the true model values are plotted as the black vertical lines**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:14:53.227501Z",
     "start_time": "2020-02-11T14:14:51.324598Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize 1D marginals for all algorithms\n",
    "%matplotlib inline\n",
    "bins = int(number_of_samples**0.4)\n",
    "\n",
    "figure, axes = plt.subplots(4, 3, figsize=(14, 14), constrained_layout=False)\n",
    "for i in range(4):\n",
    "    axes[i, 0].hist(samples_HMC[i, :], bins=bins, density=True)\n",
    "    ylim = axes[i, 0].get_ylim()\n",
    "    axes[i, 0].plot([m_true[i], m_true[i]], [0, 1], \"k\")\n",
    "    axes[i, 0].set_ylim(ylim)\n",
    "    axes[i, 0].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "    axes[i, 0].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "    axes[i, 1].hist(samples_MH[i, :], bins=bins, density=True, color='k')\n",
    "    ylim = axes[i, 1].get_ylim()\n",
    "    axes[i, 1].plot([m_true[i], m_true[i]], [0, 1], \"k\")\n",
    "    axes[i, 1].set_ylim(ylim)\n",
    "    axes[i, 1].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "    axes[i, 1].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "    axes[i, 2].hist(samples_HMC_REF[i, :], bins=bins, density=True, alpha=0.5)\n",
    "    axes[i, 2].hist(samples_MH_REF[i, :], bins=bins, density=True, alpha=0.5, color='k')\n",
    "    ylim = axes[i, 2].get_ylim()\n",
    "    axes[i, 2].plot([m_true[i], m_true[i]], [0, 1], \"k\")\n",
    "    axes[i, 2].set_ylim(ylim)\n",
    "    axes[i, 2].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "    axes[i, 2].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "\n",
    "axes[0, 0].set_title(f'HMC {samples_HMC[0,:].size} samples')\n",
    "axes[0, 1].set_title(f\"MH {samples_MH[0,:].size} samples\")\n",
    "axes[0, 2].set_title(f\"REF (MH+HMC) many samples\\n(resp. {samples_MH_REF[0,:].size:.2e} and {samples_HMC_REF[0,:].size:.2e})\")\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in quality of the results is even more pronounced when looking at higher order marginals, visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:14:55.049466Z",
     "start_time": "2020-02-11T14:14:53.263525Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the Hamiltonian Monte Carlo 2D marginals\n",
    "marginal_grid(\n",
    "    samples_HMC,\n",
    "    [0, 1, 2, 3],\n",
    "    bins=30,\n",
    "    labels=[f\"{l}\\n[{u}]\" for l, u in zip(target.labels, target.units)],\n",
    "    color_1d=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:03:09.948819Z",
     "start_time": "2020-02-11T14:03:08.102132Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the Metropolis-Hastings 2D marginals\n",
    "\n",
    "marginal_grid(\n",
    "    samples_MH,\n",
    "    [0, 1, 2, 3],\n",
    "    bins=30,\n",
    "    labels=[f\"{l}\\n[{u}]\" for l, u in zip(target.labels, target.units)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:03:11.738604Z",
     "start_time": "2020-02-11T14:03:09.949836Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the reference solution 2D marginals\n",
    "marginal_grid(\n",
    "    samples_HMC_REF, # or samples_MH_REF\n",
    "    [0, 1, 2, 3],\n",
    "    bins=30,\n",
    "    labels=[f\"{l}\\n[{u}]\" for l, u in zip(target.labels, target.units)],\n",
    "    color_1d='green',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: What does it all mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've succesfully sampled the distributions with both MH and HMC. But let's now quickly go back to the actual inverse problem itself. This is note elementary when trying to understand the HMC algorithm, but it's maybe good for our physical intuition.\n",
    "\n",
    "Looking at the marginals of the previous notebook cells, we can see that there are **strong trade-offs between medium velocity, origin time, and vertical source location**. These parameters are quite unconstratined together. Horizontal source location seems to be much better resolved.\n",
    "\n",
    "When we visualize the marginal of hypocenter location (horizontal and vertical location, **in the next notebook cell**) we can see how poorly the inference constrains vertical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:03:11.917901Z",
     "start_time": "2020-02-11T14:03:11.739673Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of possible event locations\n",
    "axes = plt.axes()\n",
    "axes.hist2d(\n",
    "    samples_HMC_REF[0, :],\n",
    "    samples_HMC_REF[1, :],\n",
    "    bins=40,\n",
    "    range=[[-5, 35], [-3, depth_limit]],\n",
    "    cmap=plt.get_cmap(\"Greys\"),\n",
    ")\n",
    "axes.invert_yaxis()\n",
    "axes.set_xlabel(\"Horizontal location [km]\")\n",
    "axes.set_ylabel(\"Vertical location [km]\")\n",
    "\n",
    "axes.plot([-5, 35], [0, 0], \"k\")\n",
    "axes.scatter(m_true[0], m_true[1], 200, label=\"True location\")\n",
    "axes.scatter(station_x, station_z, 200, label=\"Seismographs\", marker=\"v\")\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T17:01:11.337117Z",
     "start_time": "2020-02-11T17:01:11.083716Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the knowledge on the medium velocity\n",
    "prior_velocities = np.random.randn(100000) * v_std + v_mean\n",
    "posterior_velocities = samples_HMC_REF[-2, :]\n",
    "\n",
    "plt.hist(\n",
    "    posterior_velocities, bins=50, density=True, label=\"Posterior velocity\"\n",
    ")\n",
    "plt.hist(\n",
    "    prior_velocities,\n",
    "    bins=50,\n",
    "    density=True,\n",
    "    alpha=0.2,\n",
    "    color=\"k\",\n",
    "    label=\"Prior velocity\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Standard deviation prior to the experiment: {np.std(prior_velocities):.2f}\")\n",
    "print(f\"Standard deviation after to the experiment: {np.std(posterior_velocities):.2f}\")\n",
    "\n",
    "if np.std(posterior_velocities) < np.std(prior_velocities):\n",
    "    print(\"It seems like we learned something about the velocity (although very little)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: The mass matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the influence of the mass matrix, we'll try to evaluate the exact same event, but now recorded by many more stations. Intuitively, if we use more stations, our solution should become less uncertain. What this would imply for the posterior is that **the spread (multidimensionally, the covariance) would become smaller**. This is exactly what the mass matrix accounts for in HMC sampling.\n",
    "\n",
    "We now recreate the exact same HMC Markov chain (the same tuning settings) on a target with many more stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:03:12.443167Z",
     "start_time": "2020-02-11T14:03:12.259048Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a high-density receiver network\n",
    "\n",
    "# Define earthquake source properties\n",
    "source_x = 16.0\n",
    "source_z = 15.0\n",
    "origin_T = 17.0\n",
    "v_exact = 5.0\n",
    "\n",
    "# Define station coordinates of the array\n",
    "station_x = np.linspace(0, 30, 15)\n",
    "station_z = np.zeros_like(station_x)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6,6))\n",
    "axes.scatter(station_x, station_z, color='b', marker='v', s=200, label='Receivers')\n",
    "axes.scatter(source_x, source_z, color='r', marker='.', s=400, label='True location')\n",
    "axes = plt.gca()\n",
    "axes.set_xlabel(\"Horizontal location [km]\")\n",
    "axes.set_ylabel(\"Vertical location [km]\")\n",
    "axes.plot([-10, 35], [0, 0], \"k\")\n",
    "axes.set_xlim([-10,35])\n",
    "axes.set_ylim([25,-2])\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:03:12.450164Z",
     "start_time": "2020-02-11T14:03:12.444888Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the inverse problem from this high-density receiver network\n",
    "\n",
    "# Create a vector out of the true values\n",
    "m_true = np.array([source_x, source_z, origin_T, v_exact])\n",
    "\n",
    "# Calculate observed (exact) arrival times for all stations\n",
    "t_obs = forward(m_true, station_x, station_z)\n",
    "\n",
    "# Define uncertainties for the observed arrival time at each station. These are repeated for all events.\n",
    "uncertainties = np.ones_like(station_x) * 0.2\n",
    "uncertainties[0] = 0.5 # Just to recreate the first station from previous implementation\n",
    "\n",
    "assert t_obs.size == uncertainties.size\n",
    "\n",
    "# Prior information on depth and velocity\n",
    "v_mean = 4.5\n",
    "v_std = 1\n",
    "depth_limit = 25.0\n",
    "target_many_receivers = target_tt(t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T12:34:21.434123Z",
     "start_time": "2020-02-11T12:34:21.426108Z"
    }
   },
   "source": [
    "### And we start sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:03:14.577372Z",
     "start_time": "2020-02-11T14:03:12.451822Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample this inverse problem with exactly the same settings as before\n",
    "\n",
    "# Sampling using the basic HMC algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:,np.newaxis]\n",
    "\n",
    "# Tuning parameters\n",
    "# mass_matrix = np.eye(4)\n",
    "mass_matrix = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "    ], dtype=np.float32\n",
    ")  \n",
    "dt = 0.16\n",
    "nt = 40\n",
    "number_of_samples = 100\n",
    "\n",
    "# Sampling\n",
    "%time samples_HMC_many_receivers = samplers.sample_hmc_opt(target_many_receivers, m_start, nt, dt, number_of_samples, mass_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh-oh! Something that worked before, doesn't work anymore. **Almost no accepted samples.** What could this be?\n",
    "\n",
    "Well, obviously, this is due to the change in target distribution. What likely happened, is that this target is much more constrained than the previous target. The mass matrix was not updated accordingly, so sampling kind of failed (read: no samples are being generated).\n",
    "\n",
    "We could now update for this more constrained distribution by updating the mass matrix. Because the mass matrix and spread of the target distribution are inversely related, we should make the mass matrix bigger. **Try to get about 50% acceptance rate by only changing the mass matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:06:22.562704Z",
     "start_time": "2020-02-11T14:03:14.578867Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the mass matrix to account for the change in target\n",
    "\n",
    "# Sampling using the basic HMC algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:, np.newaxis]\n",
    "\n",
    "# Tuning parameters\n",
    "# mass_matrix = np.eye(4)\n",
    "\n",
    "# TODO: UPDATE!\n",
    "mass_matrix = np.array(\n",
    "    [[1, 0, 0, 0], # Horizontal source location\n",
    "     [0, 1, 0, 0], # Vertical source location\n",
    "     [0, 0, 1, 0], # Event origin time\n",
    "     [0, 0, 0, 1],], # Medium velocity\n",
    "    dtype=np.float32\n",
    ")\n",
    "dt = 0.16\n",
    "nt = 40\n",
    "number_of_samples = 10000\n",
    "\n",
    "# Sampling\n",
    "%time samples_HMC_many_receivers = samplers.sample_hmc_opt(target_many_receivers, m_start, nt, dt, number_of_samples, mass_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'upgraded' receiver network results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extended receiver network can of course tell us more about the event. Plotting the marginals for the two inferences on top of each other reveals this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T14:06:23.292611Z",
     "start_time": "2020-02-11T14:06:22.563948Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the result of the high-denisty receiver network versus two receivers\n",
    "figure, axess = plt.subplots(1, 4, figsize=(14, 4))\n",
    "\n",
    "axess[0].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "for i in range(3):\n",
    "    axess[i].hist(\n",
    "        samples_HMC_many_receivers[i, :],\n",
    "        bins=30,\n",
    "        label=\"Many receivers\",\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "    )\n",
    "    axess[i].hist(\n",
    "        samples_HMC_REF[i, :], bins=30, label=\"Two receivers\", alpha=0.5, density=True\n",
    "    )\n",
    "    axess[i].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "\n",
    "axess[-1].hist(\n",
    "    samples_HMC_many_receivers[-2, :],\n",
    "    bins=30,\n",
    "    label=\"Many receivers\",\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    ")\n",
    "axess[-1].hist(\n",
    "    samples_HMC_REF[-2, 1000:],\n",
    "    bins=30,\n",
    "    label=\"Two receivers\",\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    ")\n",
    "axess[-1].set_xlabel(\"%s [%s]\" % (target.labels[-2], target.units[-2]))\n",
    "\n",
    "axess[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "## With as many samples, HMC typically gives better results compared to MH.\n",
    "\n",
    "But we've also seen that **HMC generates samples much slower**. In Notebook 2 we see a performance comparison when HMC and MH are pitted against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code used to create the referene samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T17:34:23.029763Z",
     "start_time": "2020-02-11T17:34:23.026106Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference solution\n",
    "epsilon = 0.15\n",
    "nt = 30\n",
    "number_of_samples = 50000\n",
    "# samples_HMC_REF = samplers.sample_hmc_opt(target, m_start, nt, epsilon, number_of_samples, mass_matrix)\n",
    "epsilon = 0.3\n",
    "number_of_samples = 500000\n",
    "# samples_MH_REF = samplers.sample_mh(target, m_start, epsilon, number_of_samples);\n",
    "\n",
    "# np.save(\"ref_solutions/single_event_reference_HMC.npy\", samples_HMC_REF)\n",
    "# np.save(\"ref_solutions/single_event_reference_MH.npy\", samples_MH_REF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of the Notebook 1.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "981px",
    "left": "1648px",
    "right": "20px",
    "top": "144px",
    "width": "456px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
